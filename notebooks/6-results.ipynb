{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b8792d",
   "metadata": {},
   "source": [
    "# Learning Curves and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd8b7a",
   "metadata": {},
   "source": [
    "## baseline-2021-06-08\n",
    "\n",
    "* Start with **SGD**(lr=1e-3)\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.907\n",
    "* Dev acc:   **0.861**\n",
    "\n",
    "**Train acc.** Train accuracy is pretty far from human-level performance\n",
    "\n",
    "**Dev acc.** Dev accuracy is pretty good for a baseline.\n",
    "\n",
    "**Overall.** Good start.\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0934\n",
    "* Variance:  0.0457\n",
    "\n",
    "**Bias.** Bias is pretty high. Bias problem is more important than the variance problam at the moment.\n",
    "\n",
    "**Variance.** Variance is pretty low, probably because of the avoidable bias being high.\n",
    "\n",
    "**Overall.** Time to reduce aviodable bias.\n",
    "\n",
    "![](../learning_curves/baseline-2021-06-08.png)\n",
    "\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Introduce **momentum=0.9**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38ccde",
   "metadata": {},
   "source": [
    "## baseline-2021-06-09\n",
    "\n",
    "* Start with **SGD**(lr=1e-3, **momentum=0.9**)\n",
    "\n",
    "**Note, by default, momentum=0**.\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.997\n",
    "* Dev acc:   **0.861**\n",
    "\n",
    "**Train acc.** Train accuracy increased significantly to near-human level - we addressed avoidable bias.\n",
    "\n",
    "**Dev acc.** Dev accuracy did not change at all.\n",
    "\n",
    "**Overall.** Need to improve Dev performance\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00273\n",
    "* Variance:  0.136\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "![](../learning_curves/baseline-2021-06-09.png)\n",
    "\n",
    "Learning curves are not very smooth. There is some problem in the optimization scheme at the moment when we start training all the weights.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Try Adam Optimizer (classic choice) with default momentum parameters, same lr schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97951f7",
   "metadata": {},
   "source": [
    "## baseline-Adam-2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.998\n",
    "* Dev acc:   **0.870**\n",
    "\n",
    "Good increase in Dev accuracy.\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00156\n",
    "* Variance:  0.129\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "![](../learning_curves/baseline-Adam-2021-06-11.png)\n",
    "\n",
    "Curves got smoother.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Introduce L2 regularisation: weight_decay=1e-08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe36549",
   "metadata": {},
   "source": [
    "## Adam_wd=1e-08_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.998\n",
    "* Dev acc:   0.867\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00156\n",
    "* Variance:  0.132\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "![](../learning_curves/Adam_wd=1e-08_2021-06-11.png)\n",
    "\n",
    "Nothing really changed.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=1e-04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d91e53",
   "metadata": {},
   "source": [
    "## Adam_wd=0.0001_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.998\n",
    "* Dev acc:   0.869\n",
    "\n",
    "Dev acc decreased a bit (0.1%).\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00156\n",
    "* Variance:  0.13\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "![](../learning_curves/Adam_wd=0.0001_2021-06-11.png)\n",
    "\n",
    "Nothing really changed.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=1e-02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b9aaa",
   "metadata": {},
   "source": [
    "## Adam_wd=0.01_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.997\n",
    "* Dev acc:   0.869\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00312\n",
    "* Variance:  0.128\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "\n",
    "\n",
    "![](../learning_curves/Adam_wd=0.01_2021-06-11.png)\n",
    "\n",
    "Nothing really changed. Train curves got a bit further from the ideal performance.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a566b55",
   "metadata": {},
   "source": [
    "### Adam_wd=1_2021-06-11\n",
    "\n",
    "#### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.996\n",
    "* Dev acc:   0.865\n",
    "\n",
    "#### Bias-Varience\n",
    "\n",
    "* Bias:      0.00352\n",
    "* Variance:  0.131\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "![](../learning_curves/Adam_wd=1_2021-06-11.png)\n",
    "\n",
    "Nothing really changed.\n",
    "\n",
    "#### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1fcb46",
   "metadata": {},
   "source": [
    "## Adam_wd=10_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.939\n",
    "* Dev acc:   0.857\n",
    "\n",
    "**Train acc.** Significant drop in performance.\n",
    "\n",
    "**Dev acc.** Marginal drop in performance.\n",
    "\n",
    "**Overall.** Weight decay only hurts the train set performance, marginal (negative) effect on the dev set performance.\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0605\n",
    "* Variance:  0.082\n",
    "\n",
    "**Bias.** Bias increased significantly - maybe we started to underfit\n",
    "\n",
    "**Variance.** Variance Decreased, but only due to increase in bias.\n",
    "\n",
    "**Overall.** Weight decay does not seem to help Dev performance at all. But it did hurt the training performance.\n",
    "\n",
    "\n",
    "![](../learning_curves/Adam_wd=10_2021-06-11.png)\n",
    "\n",
    "Dev curve did not really change, while train curve flattened much faster. Less overfitting, but still want to improve the performance.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c4311",
   "metadata": {},
   "source": [
    "## Adam_wd=100_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.762\n",
    "* Dev acc:   0.764\n",
    "\n",
    "**Train acc.** Huge drop in performance.\n",
    "\n",
    "**Dev acc.** Huge drop in performance.\n",
    "\n",
    "**Overall.** Started to underfit.\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.238\n",
    "* Variance:  -0.00234\n",
    "\n",
    "**Bias.** Bias increased a lot.\n",
    "\n",
    "**Variance.** Variance is not there anymore.\n",
    "\n",
    "**Overall.** Started to underfit. Similar results to dummy classifier.\n",
    "\n",
    "![](../learning_curves/Adam_wd=100_2021-06-11.png)\n",
    "\n",
    "Flat curves, such aggressive regularization does not allow to learn.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Use **more aggressive augmentation**. Try **weight_decay=0** to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a498611",
   "metadata": {},
   "source": [
    "## strong-aug-Adam-2021-06-14\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.957\n",
    "* Dev acc:   0.864\n",
    "\n",
    "**Train acc.** There is a slight drop in traininng accuracy (harder to learn with all the augmentations).\n",
    "\n",
    "**Dev acc.** Accuracy did not really increase.\n",
    "\n",
    "**Overall.** Slightly worse train, same dev accuracy => need to do better on train set, maybe dev set performance will follow.\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0426\n",
    "* Variance:  0.093\n",
    "\n",
    "**Bias.** Bias increased compared to the baseline (w/o weight decay) Adam with simple data augmentation.\n",
    "\n",
    "**Variance.** Variance decreased, but only due to the increase in bias.\n",
    "\n",
    "**Overall.** Dev performance is still the same. Need to fit the training data better.\n",
    "\n",
    "![](../learning_curves/strong-aug-Adam-2021-06-14.png)\n",
    "\n",
    "Train loss curve flattens abruptly on epoch 9 (when lr is decreased with the scheduler). Need to give more time for training with higher lr.\n",
    "\n",
    "Also, note the flattenning of the curve on epochs 2, 3. Maybe finetuning of the last layer needs to have large (0.001) learning rate for longer: change from 2 epochs to 4 epochs with larger learning rate. Leave 2 epochs with lower learning rate to stabilize training before unfreazing all the weights.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase the number of epochs for fine-tuning both the last (4 -> 6) and all layers (10 -> 14) at higher learning rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff508e",
   "metadata": {},
   "source": [
    "##  strong-aug-longer-Adam-2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.981\n",
    "* Dev acc:   0.865\n",
    "\n",
    "**Train acc.** Longer training improved training accuracy.\n",
    "\n",
    "**Dev acc.** Accuracy increased slightly - no real change.\n",
    "\n",
    "**Overall.** Can overfit on the training set again.\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0187\n",
    "* Variance:  0.116\n",
    "\n",
    "**Bias.**  Longer training reduced bias.\n",
    "\n",
    "**Variance.** Variance increased due to the reduction in bias, performance on the dev set did not change.\n",
    "\n",
    "**Overall.** The focus should again be on reducing the variance.\n",
    "\n",
    "![](../learning_curves/strong-aug-longer-Adam-2021-06-14.png)\n",
    "\n",
    "There is no overfitting during the last-layer fine-tuning epochs. Maybe we should spend more time and effort in this area. Also, the drop during fine-tuning is pretty small, try larger learning rate to start with.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase the number of last-layer fine-tuning epochs (6->12) and the starting learning rate (0.001 -> 0.01)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb9be3",
   "metadata": {},
   "source": [
    "##  strong-aug-even-longer-Adam-2021-06-14\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.988\n",
    "* Dev acc:   0.871\n",
    "\n",
    "**Train acc.** Increased to almost 0.99\n",
    "\n",
    "**Dev acc.** Increased slightly.\n",
    "\n",
    "**Overall.** Still need to improve dev accuracy.\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0117\n",
    "* Variance:  0.117\n",
    "\n",
    "**Bias.** Bias decreased\n",
    "\n",
    "**Variance.** Variance increased a bit due to the fact that bias decreased faster.\n",
    "\n",
    "**Overall.** Need to reduce variance\n",
    "\n",
    "![](../learning_curves/strong-aug-even-longer-Adam-2021-06-14.png)\n",
    "\n",
    "**Train loss.** Decreases rapidly when all the weights are allowed to change.\n",
    "\n",
    "**Dev loss.** Only decreases while fine-tuning, stays the same afterwards.\n",
    "\n",
    "**Overall.** Loss curves converged together together while fine-tuning the last layer, but diverge straight away after all layers are allowed to change. \n",
    "\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Reduce useless training epochs (12->6, 14->10): for a long time during both stages of fine-tuning the curves are flat - no need for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c8b6e",
   "metadata": {},
   "source": [
    "##  strong-aug-not-so-long-Adam-2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.973\n",
    "* Dev acc:   **0.873**\n",
    "\n",
    "**Train acc.** Decreased slightly\n",
    "\n",
    "**Dev acc. Increased slightly - best so far!**\n",
    "\n",
    "**Overall.** Time to regularize\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0273\n",
    "* Variance:  0.0992\n",
    "\n",
    "**Bias.** Slightly increased bias.\n",
    "\n",
    "**Variance.** Lowest variance so far with good performance. Happened due to increased bias and better dev set performance.\n",
    "\n",
    "**Overall.** Best variance yet - good.\n",
    "\n",
    "![](../learning_curves/strong-aug-not-so-long-Adam-2021-06-14.png)\n",
    "\n",
    "**Train loss.** Decreases rapidly when all the weights are allowed to change.\n",
    "\n",
    "**Dev loss.** Only decreases while fine-tuning, stays the same afterwards.\n",
    "\n",
    "**Overall.** Loss curves go together while fine-tuning, but diverge straight away after all layers are allowed to change. \n",
    "\n",
    "### Next Step\n",
    "\n",
    "Time to regularize: wd=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b2230",
   "metadata": {},
   "source": [
    "##  strong-aug-not-so-long-Adam_wd=0.01_2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.975\n",
    "* Dev acc:   0.867\n",
    "\n",
    "**Train acc.** Slight increase\n",
    "\n",
    "**Dev acc.** Slight decrease\n",
    "\n",
    "**Overall.** Nothing really changed\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.025\n",
    "* Variance:  0.108\n",
    "\n",
    "**Bias.** Slight decrease\n",
    "\n",
    "**Variance.** Slight increase\n",
    "\n",
    "**Overall.** Still need to address the same overfitting problem. Nothing really changed.\n",
    "\n",
    "\n",
    "![](../learning_curves/strong-aug-not-so-long-Adam_wd=0.01_2021-06-14.png)\n",
    "\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Regularize stronger: wd=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a09ea",
   "metadata": {},
   "source": [
    "##  strong-aug-not-so-long-Adam_wd=0.1_2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.975\n",
    "* Dev acc:   0.863\n",
    "\n",
    "**Train acc.** Slight increase\n",
    "\n",
    "**Dev acc.** Slight decrease\n",
    "\n",
    "**Overall.** Same as before\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0246\n",
    "* Variance:  0.113\n",
    "\n",
    "**Bias.** Slight decrease\n",
    "\n",
    "**Variance.** Slight increase\n",
    "\n",
    "**Overall.** Same as before\n",
    "\n",
    "![](../learning_curves/strong-aug-not-so-long-Adam_wd=0.1_2021-06-14.png)\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Stronger regularization: wd=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9374231",
   "metadata": {},
   "source": [
    "##  strong-aug-not-so-long-Adam_wd=1_2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.971\n",
    "* Dev acc:   0.866\n",
    "\n",
    "**Train acc.** Slight decrease\n",
    "\n",
    "**Dev acc.** Slight increase\n",
    "\n",
    "**Overall.** Same as before\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0293\n",
    "* Variance:  0.105\n",
    "\n",
    "**Bias.** Slight increase\n",
    "\n",
    "**Variance.** Slight decrease\n",
    "\n",
    "**Overall.** Same as before\n",
    "\n",
    "![](../learning_curves/strong-aug-not-so-long-Adam_wd=1_2021-06-14.png)\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Stronger regularization: wd=10. Remove wd when fine-tuning the last layer - no overfitting there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73f689",
   "metadata": {},
   "source": [
    "### Important Decisioin\n",
    "\n",
    "**Set wd=0 while fine-tuning the last layer since we never overfit there. Save the wights.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78430a2c",
   "metadata": {},
   "source": [
    "##  strong-aug-not-so-long-Adam_wd=10_2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.927\n",
    "* Dev acc:   0.868\n",
    "\n",
    "**Train acc.** Reduced\n",
    "\n",
    "**Dev acc.** Did not change.\n",
    "\n",
    "**Overall.** We only hurt the training performanece with regularization, no increase in the Dev performance.\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0727\n",
    "* Variance:  0.059\n",
    "\n",
    "**Bias.** Bias increased.\n",
    "\n",
    "**Variance.** Variance decreased but only due to the increase in bias. Performance did not improve.\n",
    "\n",
    "**Overall.** Need to improve performance. Only managed to hurt training performance for now.\n",
    "\n",
    "![](../learning_curves/strong-aug-not-so-long-Adam_wd=10_2021-06-14.png)\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Try wd=20, slower steps now so we do not underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3392ed0",
   "metadata": {},
   "source": [
    "##  strong-aug-not-so-long-Adam_wd=20_2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.9\n",
    "* Dev acc:   0.869\n",
    "\n",
    "**Train acc.** Dropped\n",
    "\n",
    "**Dev acc.** Increased slightly\n",
    "\n",
    "**Overall.** Train performance is pulled towards Dev performance\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.1\n",
    "* Variance:  0.0305\n",
    "\n",
    "**Bias.** Increased significantly\n",
    "\n",
    "**Variance.** Decreased sigificantly, lowest up till now with good performance.\n",
    "\n",
    "**Overall.** Train performance is pulled towards Dev performance. No decrease in Dev performance.\n",
    "\n",
    "![](../learning_curves/strong-aug-not-so-long-Adam_wd=20_2021-06-14.png)\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Try wd=40."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c346e25",
   "metadata": {},
   "source": [
    "##  strong-aug-not-so-long-Adam_wd=40_2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.876\n",
    "* Dev acc:   0.863\n",
    "\n",
    "**Train acc.** Dropped seriously - equal to Dev accuracy now\n",
    "\n",
    "**Dev acc.** Dropped slightly\n",
    "\n",
    "**Overall.** \n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.124\n",
    "* Variance:  0.0137\n",
    "\n",
    "**Bias.** Bias incresed\n",
    "\n",
    "**Variance.** Variance is almost 0\n",
    "\n",
    "**Overall.** Bias takes all the gap between Bayes performance and Dev performance. Variance is minimal.\n",
    "\n",
    "![](../learning_curves/strong-aug-not-so-long-Adam_wd=40_2021-06-14.png)\n",
    "\n",
    "\n",
    "### Next Step\n",
    "\n",
    "**2 layers instead of the last fully-connected layer like in SimCLR.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c8219",
   "metadata": {},
   "source": [
    "##  2-layer-head-2021-06-15\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74256e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:standard-dl] *",
   "language": "python",
   "name": "conda-env-standard-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
