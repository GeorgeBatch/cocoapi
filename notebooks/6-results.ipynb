{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b8792d",
   "metadata": {},
   "source": [
    "# Learning Curves and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38ccde",
   "metadata": {},
   "source": [
    "## baseline-2021-06-09\n",
    "\n",
    "* SGD(lr=1e-3, **momentum=0.9**)\n",
    "* SGD(lr=1e-4, **momentum=0.9**)\n",
    "\n",
    "**Note, by default, momentum=0**\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.997\n",
    "* Dev acc:   0.861\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00273\n",
    "* Variance:  0.136\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Try Adam Optimizer with default momentum parameters, same lr schedule.\n",
    "\n",
    "![](../learning_curves/baseline-2021-06-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97951f7",
   "metadata": {},
   "source": [
    "## baseline-Adam-2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.998\n",
    "* Dev acc:   0.87\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00156\n",
    "* Variance:  0.129\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Introduce L2 regularisation: weight_decay=1e-08\n",
    "\n",
    "![](../learning_curves/baseline-Adam-2021-06-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe36549",
   "metadata": {},
   "source": [
    "## Adam_wd=1e-08_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.998\n",
    "* Dev acc:   0.867\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00156\n",
    "* Variance:  0.132\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=1e-04\n",
    "\n",
    "![](../learning_curves/Adam_wd=1e-08_2021-06-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d91e53",
   "metadata": {},
   "source": [
    "## Adam_wd=0.0001_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.998\n",
    "* Dev acc:   0.869\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00156\n",
    "* Variance:  0.13\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=1e-02\n",
    "\n",
    "![](../learning_curves/Adam_wd=0.0001_2021-06-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b9aaa",
   "metadata": {},
   "source": [
    "## Adam_wd=0.01_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.997\n",
    "* Dev acc:   0.869\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.00312\n",
    "* Variance:  0.128\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=1\n",
    "\n",
    "![](../learning_curves/Adam_wd=0.01_2021-06-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a566b55",
   "metadata": {},
   "source": [
    "### Adam_wd=1_2021-06-11\n",
    "\n",
    "#### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.996\n",
    "* Dev acc:   0.865\n",
    "\n",
    "#### Bias-Varience\n",
    "\n",
    "* Bias:      0.00352\n",
    "* Variance:  0.131\n",
    "\n",
    "**Bias.** Bias is very low, no need to do anything here.\n",
    "\n",
    "**Variance.** Need to reduce variance, model is overfitting to the train set.\n",
    "\n",
    "#### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=10\n",
    "\n",
    "![](../learning_curves/Adam_wd=1_2021-06-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1fcb46",
   "metadata": {},
   "source": [
    "## Adam_wd=10_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.939\n",
    "* Dev acc:   0.857\n",
    "\n",
    "**Train acc.** Significant drop in performance.\n",
    "\n",
    "**Dev acc.** Marginal drop in performance.\n",
    "\n",
    "**Overall.** Weight decay only hurts the train set performance, marginal (negative) effect on the dev set performance.\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0605\n",
    "* Variance:  0.082\n",
    "\n",
    "**Bias.** Bias increased significantly - maybe we started to underfit\n",
    "\n",
    "**Variance.** Variance Decreased, but only due to increase in bias.\n",
    "\n",
    "**Overall.** Weight decay does not seem to help at all.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase L2 regularisation: weight_decay=100\n",
    "\n",
    "![](../learning_curves/Adam_wd=10_2021-06-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c4311",
   "metadata": {},
   "source": [
    "## Adam_wd=100_2021-06-11\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.762\n",
    "* Dev acc:   0.764\n",
    "\n",
    "**Train acc.** Huge drop in performance.\n",
    "\n",
    "**Dev acc.** Huge drop in performance.\n",
    "\n",
    "**Overall.** Started to underfit.\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.238\n",
    "* Variance:  -0.00234\n",
    "\n",
    "**Bias.** Bias increased a lot.\n",
    "\n",
    "**Variance.** Variance is not there anymore.\n",
    "\n",
    "**Overall.** Started to underfit.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Use **more aggressive augmentation**. Try w/o weight decay to start with.\n",
    "\n",
    "![](../learning_curves/Adam_wd=100_2021-06-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a498611",
   "metadata": {},
   "source": [
    "## strong-aug-Adam-2021-06-14\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.957\n",
    "* Dev acc:   0.864\n",
    "\n",
    "**Train acc.** There is a slight drop in traininng accuracy (harder to learn with all the augmentations).\n",
    "\n",
    "**Dev acc.** Accuracy did not really increase.\n",
    "\n",
    "**Overall.** Slightly worse train, same dev accuracy => need to do better on train set, maybe dev set performance will follow.\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0426\n",
    "* Variance:  0.093\n",
    "\n",
    "**Bias.** Bias increased compared to the baseline (w/o weight decay) Adam with simple data augmentation.\n",
    "\n",
    "**Variance.** Variance decreased, but only due to the increase in bias.\n",
    "\n",
    "**Overall.** Dev performance is still the same. Need to fit the training data better.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Increase the number of epochs for fine-tuning both the last and all layers. \n",
    "\n",
    "![](../learning_curves/strong-aug-Adam-2021-06-14.png)\n",
    "\n",
    "Note the flattenning of the curve on epochs 2, 3. Maybe finetuning of the last layer needs to have large (0.001) learning rate for longer: change from 2 epochs to 4 epochs with larger learning rate. Leave 2 epochs with lower learning rate to stabilize training before unfreazing all the weights. Also, will try to start with "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff508e",
   "metadata": {},
   "source": [
    "##  strong-aug-longer-Adam-2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.981\n",
    "* Dev acc:   0.865\n",
    "\n",
    "**Train acc.** Longer training improved training accuracy.\n",
    "\n",
    "**Dev acc.** Accuracy increased slightly - no real change.\n",
    "\n",
    "**Overall.** Can overfit on the training set again.\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0187\n",
    "* Variance:  0.116\n",
    "\n",
    "**Bias.**  Longer training reduced bias.\n",
    "\n",
    "**Variance.** Variance increased due to the reduction in bias, performance on the dev set did not change.\n",
    "\n",
    "**Overall.** The focus should again be on reducing the variance.\n",
    "\n",
    "![](../learning_curves/strong-aug-longer-Adam-2021-06-14.png)\n",
    "\n",
    "### Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb9be3",
   "metadata": {},
   "source": [
    "##  strong-aug-even-longer-Adam-2021-06-14\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.988\n",
    "* Dev acc:   0.871\n",
    "\n",
    "**Train acc.** Increased to almost 0.99\n",
    "\n",
    "**Dev acc.** Increased slightly.\n",
    "\n",
    "**Overall.** Still need to improve dev accuracy.\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0117\n",
    "* Variance:  0.117\n",
    "\n",
    "**Bias.** Bias decreased\n",
    "\n",
    "**Variance.** Variance increased a bit due to the fact that bias decreased faster.\n",
    "\n",
    "**Overall.** Need to reduce variance\n",
    "\n",
    "![](../learning_curves/strong-aug-even-longer-Adam-2021-06-14.png)\n",
    "\n",
    "**Train loss.** Decreases rapidly when all the weights are allowed to change.\n",
    "\n",
    "**Dev loss.** Only decreases while fine-tuning, stays the same afterwards.\n",
    "\n",
    "**Overall.** Loss curves go together while fine-tuning, but diverge straight away after all layers are allowed to change. \n",
    "\n",
    "\n",
    "### Next Step\n",
    "\n",
    "Reduce useless training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c8b6e",
   "metadata": {},
   "source": [
    "##  strong-aug-not-so-long-Adam-2021-06-14\n",
    "\n",
    "### Results\n",
    "\n",
    "* Bayes acc: 1.0\n",
    "* Train acc: 0.973\n",
    "* Dev acc:   0.873\n",
    "\n",
    "**Train acc.** Decreased slightly\n",
    "\n",
    "**Dev acc.** Increased slightly - best so far\n",
    "\n",
    "**Overall.** Time to regularize\n",
    "\n",
    "\n",
    "### Bias-Varience\n",
    "\n",
    "* Bias:      0.0273\n",
    "* Variance:  0.0992\n",
    "\n",
    "**Bias.** Slightly increased bias.\n",
    "\n",
    "**Variance.** Lowest variance so far with good performance. Happened due to increased bias and better dev set performance.\n",
    "\n",
    "**Overall.** Best variance yet - good.\n",
    "\n",
    "![](../learning_curves/strong-aug-not-so-long-Adam-2021-06-14.png)\n",
    "\n",
    "**Train loss.** Decreases rapidly when all the weights are allowed to change.\n",
    "\n",
    "**Dev loss.** Only decreases while fine-tuning, stays the same afterwards.\n",
    "\n",
    "**Overall.** Loss curves go together while fine-tuning, but diverge straight away after all layers are allowed to change. \n",
    "\n",
    "### Next Step\n",
    "\n",
    "Time to regularize: wd=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d2c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:standard-dl] *",
   "language": "python",
   "name": "conda-env-standard-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
